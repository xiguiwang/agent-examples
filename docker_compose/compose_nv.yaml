# SPDX-License-Identifier: Apache-2.0

services:
  vllm-service:
    image: vllm/vllm-openai:v0.9.2
    container_name: vllm-openai
    ports:
      - "8000:8000"
    volumes:
      - "/disk/.cache/huggingface:/root/.cache/huggingface"
      - "/dev/dri/by-path:/dev/dri/by-path"
    shm_size: 1g
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      #HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      HF_TOKEN: ${HF_TOKEN}
      LLM_MODEL_ID: Qwen/Qwen3-4B
      HF_OFFLINE: 1
      TRANSFORMERS_OFFLINE: 1
      VLLM_TORCH_PROFILER_DIR: "/mnt"
    #command: --model "Qwen/Qwen3-4B" --host 0.0.0.0 --port 8000
    command: --model "Qwen/Qwen3-4B" --host 0.0.0.0 --port 8000 --dtype=half --enable-auto-tool-choice --tool-call-parser hermes
    ipc: host
    restart: always

networks:
  default:
    driver: bridge
